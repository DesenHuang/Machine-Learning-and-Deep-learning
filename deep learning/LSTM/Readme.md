Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。

![avater](LSTM.png)

[RNN反向传播推导](https://zhuanlan.zhihu.com/p/28806793)

[RNN梯度消失和爆炸](https://zhuanlan.zhihu.com/p/28687529)

[LSTM如何缓解梯度消失和爆炸](https://zhuanlan.zhihu.com/p/28749444) --> 公式被缩减部分有错误参照楼上正确版本，但总体思路清晰。

梯度消失应该存在的可能性小一些，因为梯度是逐时刻累加。LSTM很好的缓解了梯度爆炸问题（传统的方法有梯度截断防止梯度爆炸也能取得很好的效果），但是对于梯度消失LSTM还是没怎么改善吧。离的远的时刻的误差传播梯度确实贡献小很多，长期依赖还是不能很好的被捕捉。

![avater](LSTM原文.png)