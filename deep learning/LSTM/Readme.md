Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。

![avater](LSTM.png)

[RNN反向传播推导](https://zhuanlan.zhihu.com/p/28806793)

[RNN梯度消失和爆炸](https://zhuanlan.zhihu.com/p/28687529)

[LSTM如何缓解梯度消失和爆炸](https://zhuanlan.zhihu.com/p/28749444) --> 公式被缩减部分有错误参照楼上正确版本，但总体思路清晰。

[LSTM反向传播推导硬核](https://www.cnblogs.com/pinard/p/6519110.html)

梯度消失应该存在的可能性小一些，因为梯度是逐时刻累加。LSTM很好的缓解了梯度爆炸问题（传统的方法有梯度截断防止梯度爆炸也能取得很好的效果），但是对于梯度消失LSTM还是没怎么改善吧。离的远的时刻的误差传播梯度确实贡献小很多，长期依赖还是不能很好的被捕捉。

![avater](LSTM原文.png)

‘梯度消失’其实在DNN和RNN中意义不一样，DNN中梯度消失指的是误差无法传递回浅层，导致浅层的参数无法更新；而RNN中的梯度消失是指较早时间步所贡献的更新值，无法被较后面的时间步获取，导致后面时间步进行误差更新的时候，采用的只是附近时间步的数据。【即，RNN中参数还是可以更新的，但是没办法满足它最开始的假设，利用到较早信息。】