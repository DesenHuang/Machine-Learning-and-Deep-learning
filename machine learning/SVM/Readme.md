## 支持向量机(手推)

* (学习链接)[https://zhuanlan.zhihu.com/p/31886934]
SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对于线性可分的数据集来说，这样的超平面有无穷多个（即感知机），但是几何间隔最大的分离超平面却是唯一的。

其损失函数为：
$$ J(\Theta ) = \frac{1}{2}\left \| \Theta  \right \|^{2}+C \underset{i}{\sum}max(0,1-y_{i}(\Theta ^{T}x_{i}+b)) $$

## LR与SVM的区别与联系

LR的损失函数是Cross entropy loss，svm的损失函数是Hinge loss，两个模型都属于线性分类器，而且性能相当。区别在于

* LR的输出具有自然的概率意义，而SVM的输出不具有概率意义；
* LR适合于大样本学习，SVM适合于小样本学习。

换用其他的Loss函数的话，SVM就不再是SVM了。正是因为Hinge Loss的零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这才是支持向量机最大的优势所在，对训练样本数目的依赖大大减少，而且提高了训练效率。